#server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_community.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import torch
import re

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class PromptRequest(BaseModel):
    """
    Data model to represent the structure of the request body.
    Contains a prompt string and an optional max_tokens integer (default: 8000).
    """
    prompt: str
    max_tokens: int = 8000

app = FastAPI()

# Custom callback handler to implement stopping condition
class StoppingConditionHandler(StreamingStdOutCallbackHandler):
    """
    Custom callback handler to manage the generation of text with the ability to stop when a certain condition is met.
    This handler accumulates the generated text and checks if a stopping condition (e.g., 8 complete JSON objects) has been met.
    """
    def __init__(self):
        self.generated_text = ""
        self.stop_generation = False

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        Callback function that is triggered when a new token is generated by the LLM.
        Accumulates the generated text and checks if the stopping condition is met.
        """
        self.generated_text += token
        if self.check_stopping_condition():
            self.stop_generation = True

    def check_stopping_condition(self) -> bool:
        """
        Checks if the stopping condition has been met, which in this case is the presence of 8 complete JSON objects.
        Returns True if the condition is met, otherwise False.
        """
        json_objects = re.findall(r'\{[^{}]*\}', self.generated_text)
        return len(json_objects) >= 8

# Initialize the model
n_gpu_layers = 32
n_batch = 1024
stopping_condition_handler = StoppingConditionHandler()
callback_manager = CallbackManager([stopping_condition_handler])
model_path = "C:/scoo/sem5/FYP/VSC/capybarahermes-2.5-mistral-7b.Q5_K_M.gguf"

llm = LlamaCpp(
    model_path=model_path,
    n_gpu_layers=n_gpu_layers,
    n_batch=n_batch,
    max_tokens=8000,
    n_ctx=6000,
    temperature=0.1,
    top_p=0.1,
    f16_kv=True,
    callback_manager=callback_manager,
    verbose=False,
    n_threads=12
)

# Global flag for stopping the generation
should_stop = False

@app.post("/stop/")
async def stop_generation():
    """
    Endpoint to stop the ongoing generation process.
    Sets the global should_stop flag to True, which will be checked during the text generation.
    Returns a message confirming that the generation will be stopped.
    """
    global should_stop
    should_stop = True
    return {"message": "Generation will be stopped"}

def check_should_stop():
    """
    Helper function to check if the should_stop flag is set to True.
    If True, the flag is reset to False, and the function returns True indicating the generation should stop.
    Returns False if the flag is not set.
    """
    global should_stop
    if should_stop:
        should_stop = False  # Reset the flag after stopping
        return True
    return False

@app.post("/generate/")
async def generate_response(request: PromptRequest):
    """
    Endpoint to handle the generation of responses based on the provided prompt.
    Uses the LlamaCpp model to generate text based on the prompt and stops if the user requests to stop.
    Returns the generated response formatted as a string containing up to 8 JSON objects.
    Raises an HTTPException if an error occurs during the process.
    """
    try:
        stopping_condition_handler.generated_text = ""
        stopping_condition_handler.stop_generation = False

        response = llm.invoke(
            request.prompt,
            max_tokens=request.max_tokens
        )

        if check_should_stop():
            return {"response": "Audit Stopped by User"}

        json_objects = re.findall(r'\{[^{}]*\}', response)
        formatted_response = "\n".join(json_objects[:8])  # Limit to 8 objects

        return {"response": formatted_response.strip()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    """
    Basic endpoint to check if the server is running.
    Returns a simple message indicating the server status.
    """
    return {"message": "Server is running"}

